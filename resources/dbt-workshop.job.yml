resources:
  jobs:
    dbt-workshop_job:
      name: dbt-workshop_job

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      #email_notifications:
      #  on_failure:
      #    - your_email@example.com
 
      job_clusters:
        - job_cluster_key: dbt_CLI
          new_cluster:
            spark_version: 15.4.x-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            azure_attributes:
              first_on_demand: 1
              availability: ON_DEMAND_AZURE
              spot_bid_max_price: -1
            node_type_id: Standard_D4s_v3
            custom_tags:
              ResourceClass: SingleNode
              activity: daily-load
            spark_env_vars:
              PYSPARK_PYTHON: /databricks/python3/bin/python3
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER            
            num_workers: 0

      tasks:
        - task_key: dbt
          job_cluster_key: dbt_CLI 

          dbt_task:
            project_directory: ../
            # The default schema, catalog, etc. are defined in ../dbt_profiles/profiles.yml
            profiles_directory: dbt_profiles/
            commands:
            # The dbt commands to run (see also the dev/prod profiles in dbt_profiles/profiles.yml)
            - 'dbt deps --target=${bundle.target}'
            - 'dbt seed --target=${bundle.target}'
            - 'dbt build --target=${bundle.target}'
          max_retries: 1
          min_retry_interval_millis: 60000

          libraries:
          - pypi:
              package: dbt-databricks>=1.8.0,<2.0.0
         
      permissions:
        - group_name: dbx-admins
          level: CAN_MANAGE
 
      parameters:
        - name: env
          default: ${bundle.target}
